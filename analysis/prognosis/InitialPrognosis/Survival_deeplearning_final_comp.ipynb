{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../deepsurv')\n",
    "import deepsurv\n",
    "\n",
    "#from deepsurv_logger import DeepSurvLogger, TensorboardLogger\n",
    "import utils\n",
    "import viz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lasagne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eln_clin_demo_comp = [0]+list(range(153,180))\n",
    "eln_cyto_gen_comp = list(range(171))\n",
    "eln_cyto_comp = [0] + list(range(84,171))\n",
    "eln_gen_comp = list(range(84)) + list(range(153,171))\n",
    "clin_demo_comp = list(range(153,180))\n",
    "cyto_gen_comp = list(range(1,171))\n",
    "cyto_comp = list(range(84,171))\n",
    "gen_comp = list(range(1,84))+list(range(153,171))\n",
    "clin_demo_cyto_gen_comp = list(range(1,180))\n",
    "comp = list(range(153,171))\n",
    "clin_demo_cyto_gen = list(range(1,153))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,clin_demo_cyto_gen+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_deepsurv_ds(df, event_col = 'os_status', time_col = 'os'):\n",
    "    # Extract the event and time columns as numpy arrays\n",
    "    e = df[event_col].values.astype(np.int32)\n",
    "    t = df[time_col].values.astype(np.float32)\n",
    "\n",
    "    # Extract the patient's covariates as a numpy array\n",
    "    x_df = df.drop([event_col, time_col], axis = 1)\n",
    "    x = x_df.values.astype(np.float32)\n",
    "    #x=x_df\n",
    "    \n",
    "    # Return the deep surv dataframe\n",
    "    return {\n",
    "        'x' : x,\n",
    "        'e' : e,\n",
    "        't' : t\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:43:37,001 - Training step 0/5000    |                         | - loss: 6.3432 - ci: 0.7797\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:04,102 - Training step 250/5000  |*                        | - loss: 6.3269 - ci: 0.7805\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:31,373 - Training step 500/5000  |**                       | - loss: 6.3218 - ci: 0.7816\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:44:58,861 - Training step 750/5000  |***                      | - loss: 6.3122 - ci: 0.7825\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:26,538 - Training step 1000/5000 |*****                    | - loss: 6.3091 - ci: 0.7835\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:45:54,465 - Training step 1250/5000 |******                   | - loss: 6.2996 - ci: 0.7845\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:22,080 - Training step 1500/5000 |*******                  | - loss: 6.3089 - ci: 0.7857\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:46:50,857 - Training step 1750/5000 |********                 | - loss: 6.2895 - ci: 0.7867\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:19,042 - Training step 2000/5000 |**********               | - loss: 6.2777 - ci: 0.7877\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:47:47,094 - Training step 2250/5000 |***********              | - loss: 6.2918 - ci: 0.7888\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:16,775 - Training step 2500/5000 |************             | - loss: 6.2635 - ci: 0.7900\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:48:46,427 - Training step 2750/5000 |*************            | - loss: 6.2751 - ci: 0.7912\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:16,319 - Training step 3000/5000 |***************          | - loss: 6.2577 - ci: 0.7924\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:49:46,049 - Training step 3250/5000 |****************         | - loss: 6.2471 - ci: 0.7936\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:16,456 - Training step 3500/5000 |*****************        | - loss: 6.2595 - ci: 0.7948\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:50:44,943 - Training step 3750/5000 |******************       | - loss: 6.2468 - ci: 0.7961\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:12,809 - Training step 4000/5000 |********************     | - loss: 6.2521 - ci: 0.7973\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:51:41,538 - Training step 4250/5000 |*********************    | - loss: 6.2450 - ci: 0.7985\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:12,187 - Training step 4500/5000 |**********************   | - loss: 6.2434 - ci: 0.7997\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:52:44,428 - Training step 4750/5000 |***********************  | - loss: 6.2510 - ci: 0.8010\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "2019-06-04 16:53:12,258 - Finished Training with 5000 iterations in 575.45s\n",
      "[(0, 0.6512132472131474), (250, 0.6503653457692212), (500, 0.6505648519913215), (750, 0.6500660864360708), (1000, 0.6499164567694955), (1250, 0.6498167036584453), (1500, 0.6493179381031946), (1750, 0.649243123269907), (2000, 0.6488441108257064), (2250, 0.6488939873812315), (2500, 0.6486196663258436), (2750, 0.6481957156038805), (3000, 0.64799620938178), (3250, 0.6474475672710043), (3500, 0.6468739868824659), (3750, 0.6464749744382653), (4000, 0.6459013940497269), (4250, 0.6454774433277638), (4500, 0.6449537394947505), (4750, 0.6448539863837004)]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,cyto_gen_comp+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)\n",
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')\n",
    "hid_layers=[100,100,10]\n",
    "#network=[None]*2\n",
    "#metrics=[None]*2\n",
    "for i in range(1):\n",
    "\n",
    "    hyperparams = {\n",
    "        'L2_reg': 1.0,\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.4,\n",
    "        'hidden_layers_sizes': [100,100],\n",
    "        'learning_rate': 1e-3,\n",
    "        'lr_decay': 0.001,\n",
    "        'momentum': 0.9,\n",
    "        'n_in': train_data['x'].shape[1],\n",
    "        'activation':'rectify'\n",
    "        #'standardize': True\n",
    "    }\n",
    "    np.random.seed(17)\n",
    "    #network[i] = deepsurv.DeepSurv(**hyperparams)\n",
    "    #network[i].restored_update_params = None \n",
    "    logger = deepsurv.deepsurv_logger.TensorboardLogger(name = 'DeepSurvLogger', logdir = './logs/tensorboard/')\n",
    "    np.random.seed(17)\n",
    "    #update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \n",
    "    #update_fn=lasagne.updates.rmsprop\n",
    "    n_epochs = 5000\n",
    "    val_frequency=250\n",
    "    patience=10000 # minimum number of epochs to train for\n",
    "    metrics[i] =  network[i].train(train_data,val_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn,patience=patience,validation_frequency=val_frequency)\n",
    "    print (metrics[i]['valid_c-index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6512132472131474),\n",
       " (250, 0.6503653457692212),\n",
       " (500, 0.6505648519913215),\n",
       " (750, 0.6500660864360708),\n",
       " (1000, 0.6499164567694955),\n",
       " (1250, 0.6498167036584453),\n",
       " (1500, 0.6493179381031946),\n",
       " (1750, 0.649243123269907),\n",
       " (2000, 0.6488441108257064),\n",
       " (2250, 0.6488939873812315),\n",
       " (2500, 0.6486196663258436),\n",
       " (2750, 0.6481957156038805),\n",
       " (3000, 0.64799620938178),\n",
       " (3250, 0.6474475672710043),\n",
       " (3500, 0.6468739868824659),\n",
       " (3750, 0.6464749744382653),\n",
       " (4000, 0.6459013940497269),\n",
       " (4250, 0.6454774433277638),\n",
       " (4500, 0.6449537394947505),\n",
       " (4750, 0.6448539863837004)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[0]['valid_c-index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6442305294396369 0.663980091514811 {'n_in': 170, 'learning_rate': 1e-05, 'hidden_layers_sizes': [100, 100], 'lr_decay': 0.001, 'momentum': 0.9, 'L2_reg': 1.0, 'L1_reg': 0.0, 'activation': 'rectify', 'dropout': 0.4, 'batch_norm': True, 'standardize': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print (network[i].get_concordance_index(x=val_data['x'],t=val_data['t'],e=val_data['e']),network[i].get_concordance_index(x=test_data['x'],t=test_data['t'],e=test_data['e']),network[i].hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[i] =  network[i].train(train_data,val_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn,patience=patience,validation_frequency=val_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,clin_demo_cyto_gen_comp+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)\n",
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')\n",
    "hid_layers=[[100,100,10],[200,500,10],[250,100,100],[500,100,100]]\n",
    "drop=[0.2,0.2,0.2,0.2]\n",
    "network1=[None]*4\n",
    "metrics1=[None]*4\n",
    "for i in range(4):\n",
    "\n",
    "    hyperparams = {\n",
    "        'L2_reg': 10.0,\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.4,\n",
    "        'hidden_layers_sizes': hid_layers[i],\n",
    "        'learning_rate': 1e-05,\n",
    "        'lr_decay': 0.001,\n",
    "        'momentum': 0.9,\n",
    "        'n_in': train_data['x'].shape[1],\n",
    "        'activation':'rectify'\n",
    "        #'standardize': True\n",
    "    }\n",
    "    np.random.seed(17)\n",
    "    network1[i] = deepsurv.DeepSurv(**hyperparams)\n",
    "    network1[i].restored_update_params = None \n",
    "    logger = deepsurv.deepsurv_logger.TensorboardLogger(name = 'DeepSurvLogger', logdir = './logs/tensorboard/')\n",
    "    np.random.seed(17)\n",
    "    #update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \n",
    "    update_fn=lasagne.updates.rmsprop\n",
    "    n_epochs = 12000\n",
    "    val_frequency=250\n",
    "    patience=10000 # minimum number of epochs to train for\n",
    "    metrics1[i] =  network1[i].train(train_data,val_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn,patience=patience,validation_frequency=val_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

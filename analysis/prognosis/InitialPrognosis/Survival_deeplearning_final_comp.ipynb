{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../deepsurv')\n",
    "import deepsurv\n",
    "\n",
    "#from deepsurv_logger import DeepSurvLogger, TensorboardLogger\n",
    "import utils\n",
    "import viz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lasagne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eln_clin_demo_comp = [0]+list(range(153,180))\n",
    "eln_cyto_gen_comp = list(range(171))\n",
    "eln_cyto_comp = [0] + list(range(84,171))\n",
    "eln_gen_comp = list(range(84)) + list(range(153,171))\n",
    "clin_demo_comp = list(range(153,180))\n",
    "cyto_gen_comp = list(range(1,171))\n",
    "cyto_comp = list(range(84,171))\n",
    "gen_comp = list(range(1,84))+list(range(153,171))\n",
    "clin_demo_cyto_gen_comp = list(range(1,180))\n",
    "comp = list(range(153,171))\n",
    "clin_demo_cyto_gen = list(range(1,153))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,clin_demo_cyto_gen+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_deepsurv_ds(df, event_col = 'os_status', time_col = 'os'):\n",
    "    # Extract the event and time columns as numpy arrays\n",
    "    e = df[event_col].values.astype(np.int32)\n",
    "    t = df[time_col].values.astype(np.float32)\n",
    "\n",
    "    # Extract the patient's covariates as a numpy array\n",
    "    x_df = df.drop([event_col, time_col], axis = 1)\n",
    "    x = x_df.values.astype(np.float32)\n",
    "    #x=x_df\n",
    "    \n",
    "    # Return the deep surv dataframe\n",
    "    return {\n",
    "        'x' : x,\n",
    "        'e' : e,\n",
    "        't' : t\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:44:28,558 - Training step 7250/12000  |***************          | - loss: 7.7765 - ci: 0.7166\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:05,079 - Training step 7500/12000  |***************          | - loss: 7.7310 - ci: 0.7184\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:45:41,938 - Training step 7750/12000  |****************         | - loss: 7.7202 - ci: 0.7199\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n",
      "2019-06-04 12:46:17,020 - Training step 8000/12000  |****************         | - loss: 7.7125 - ci: 0.7218\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,cyto_gen_comp+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)\n",
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')\n",
    "hid_layers=[[100,100],[100,100,10]]\n",
    "network=[None]*2\n",
    "metrics=[None]*2\n",
    "for i in range(2):\n",
    "\n",
    "    hyperparams = {\n",
    "        'L2_reg': 1.0,\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.4,\n",
    "        'hidden_layers_sizes': hid_layers[i],\n",
    "        'learning_rate': 1e-05,\n",
    "        'lr_decay': 0.001,\n",
    "        'momentum': 0.9,\n",
    "        'n_in': train_data['x'].shape[1],\n",
    "        'activation':'rectify'\n",
    "        #'standardize': True\n",
    "    }\n",
    "    np.random.seed(17)\n",
    "    network[i] = deepsurv.DeepSurv(**hyperparams)\n",
    "    network[i].restored_update_params = None \n",
    "    logger = deepsurv.deepsurv_logger.TensorboardLogger(name = 'DeepSurvLogger', logdir = './logs/tensorboard/')\n",
    "    np.random.seed(17)\n",
    "    #update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \n",
    "    update_fn=lasagne.updates.rmsprop\n",
    "    n_epochs = 12000\n",
    "    val_frequency=250\n",
    "    patience=10000 # minimum number of epochs to train for\n",
    "    metrics[i] =  network[i].train(train_data,val_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn,patience=patience,validation_frequency=val_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print (network[i].get_concordance_index(x=val_data['x'],t=val_data['t'],e=val_data['e']),network[i].get_concordance_index(x=test_data['x'],t=test_data['t'],e=test_data['e']),network[i].hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('df_prognosis_features_ready_final_component.tsv').iloc[:,clin_demo_cyto_gen_comp+[180,181]]\n",
    "train,test = train_test_split(train_df,test_size=0.2,random_state=17)\n",
    "train,val = train_test_split(train,test_size=0.2,random_state=17)\n",
    "### Min Max Scaler Normalization and good format of the data to feed the MLP network\n",
    "scale = preprocessing.MinMaxScaler().fit(train)\n",
    "df_train = pd.DataFrame(scale.transform(train.values), columns=train.columns, index=train.index)\n",
    "df_val = pd.DataFrame(scale.transform(val.values), columns=val.columns, index=val.index)\n",
    "df_test = pd.DataFrame(scale.transform(test.values), columns=test.columns, index=test.index)\n",
    "train_data = dataframe_to_deepsurv_ds(df_train, event_col = 'os_status', time_col= 'os')\n",
    "val_data = dataframe_to_deepsurv_ds(df_val, event_col = 'os_status', time_col= 'os')\n",
    "test_data = dataframe_to_deepsurv_ds(df_test, event_col = 'os_status', time_col= 'os')\n",
    "hid_layers=[[100,100,10],[200,500,10],[250,100,100],[500,100,100]]\n",
    "drop=[0.2,0.2,0.2,0.2]\n",
    "network1=[None]*4\n",
    "metrics1=[None]*4\n",
    "for i in range(4):\n",
    "\n",
    "    hyperparams = {\n",
    "        'L2_reg': 10.0,\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.4,\n",
    "        'hidden_layers_sizes': hid_layers[i],\n",
    "        'learning_rate': 1e-05,\n",
    "        'lr_decay': 0.001,\n",
    "        'momentum': 0.9,\n",
    "        'n_in': train_data['x'].shape[1],\n",
    "        'activation':'rectify'\n",
    "        #'standardize': True\n",
    "    }\n",
    "    np.random.seed(17)\n",
    "    network1[i] = deepsurv.DeepSurv(**hyperparams)\n",
    "    network1[i].restored_update_params = None \n",
    "    logger = deepsurv.deepsurv_logger.TensorboardLogger(name = 'DeepSurvLogger', logdir = './logs/tensorboard/')\n",
    "    np.random.seed(17)\n",
    "    #update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \n",
    "    update_fn=lasagne.updates.rmsprop\n",
    "    n_epochs = 12000\n",
    "    val_frequency=250\n",
    "    patience=10000 # minimum number of epochs to train for\n",
    "    metrics1[i] =  network1[i].train(train_data,val_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn,patience=patience,validation_frequency=val_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
